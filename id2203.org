#+TITLE: ID2203
#+AUTHOR: Steven Shidi Zhou
#+DESCRIPTION: Lecture notes on Course ID2203 Advanced Distributed Systems
* STRT Lecture 1 - introduction to Distributed Systems
Distributed System definition: A set of *nodes*, connected by a *network*, which appear to its users as a *single* coherent system.
** Core problems in Distributed Systems
*** Consensus/Agreement Problem
  + All nodes/processes /propose/ a /value/.
  + Some nodes (non correct nodes) might crash & stop responding
*** Broadcast Problem
**** Atomic Broadcast
+ A node broadcasts a message
+ If sender correct, all correct nodes deliver message
+ All correct nodes deliver the *same* messages
+ Messages delivered in the *same order*

Atomic broadcast can be used to provide fault tolerance to solve Replicated State Machines (RSM)

Atomic broadcast can be used to solve Consensus! i.e. Every node broadcasts its proposal:
    1. Decide on the *first* received proposal
    2. Messages received in same order, thus all nodes will decide the same value

*Atomic Broadcast is equivalent to Consensus*
** Models of Distributed Systems
*** Asynchronous System Model
+ No bound on time to deliver a message
+ No bound on time to compute
+ Clocks are not synchronized

  *Internet is essentially asynchronous*.

  *Consensus* /cannot/ be solved in asynchronous system if node crashes can happen. you can not find out if a process has failed or just slow.
*** Synchronous System
+ Known bound on time to deliver a message (latency)
+ Known bound on time to compute
+ Known lower and upper bounds in physical clock drift rate.

  Example: Embedded systems(shared clock), Multi-core computers

  *Consensus* is */solvable/* in synchronous system with up to N-1 crashes.
*** Partially synchronous System
+ Initially system is asynchronous
+ Eventually the system becomes synchronous.

  *Consensus is solvable* in partially synchronous system with up to N/2 crashes
*** Timed Asynchronous System
+ No bound on time to deliver a message
+ No bound on time to compute
+ Clocks have known clock drift rate
** Byzantine Faults
Some processes might behave arbitrarily:
+ Sending wrong information
+ Omit message...

*** Byzantine algorithms that tolerate such faults
+ Only tolerate up to 1/3 Byzantine processes
+ Non-Byzantine algorithms can often tolerate 1/2 nodes in the asynchronous model
* STRT Lecture 2 - Basic Abstrctions
** Specification of a Service
*** Correctness
**** Safety
Properties that state that nothing bad *ever* happens. (Often involves the word "never", "at most", "cannot"...)

Safety can only be:
- *satisfied* in infinite time. (you are never safe)
- *violated* in finite time (when the bad happens)

Somtimes called "partial correctness"
**** Liveness
Properties that something good eventually happens. (Often involves the word "eventually" or "must")

Liveness can only be:
- *satisfied* in finite time. (when the good happens)
- *violated* in infinite time. (there is always hope)

Liveness is often just "*Termination*"
*** Models / Assumptions
**** Process failures
***** Crash-stop
Process stops taking steps. (i.e  Not sending messages, nor receiving messages)

Hence, process do not recover
***** Omissions
Process omits sending or receiving messages:
****** Send omission
Not sending messages the process has to send according to its algorithm.
****** Receive omission
Not receiving messages that have been sent to the process
***** Crash-revocery
The process might crash (same as above)

It may *recover* after crashing

Has access to *stable storage*

****** Failure is different in crash-recovery model
A process is *faulty* in an execution if:
- It crashes and *never* recovers, or
- It crashes and recovers *infinitely often* (Unstable)

Hence, a *correct process* may crash and recover *As long as it is a finite number of time*
***** Byzantine failures
A process may behave arbitrarily:
- Sending messages not specified by its algorithm
- Updating its state as not specified by its algorithm

Maybe behave maliciouly, attacking the system
***** Fault-tolerance hierarchy
Byzantine tolerance -> Crash-recovery tolerance -> Omission -> Crash-stop
**** Channel failures
***** Fair-Loss Links
Channels that delivers any message send with non-zero probability (No network partitions)
****** Properties
- *FL1. Fair-loss*: If m is sent infinitely often by P_i to P_j and neither crash, then m is delivered infinitely often by P_j *(Liveness)*
- *FL2. Finite duplication*: If a message m is sent a finite number of times by P_i to P_j, then it is delivered at most a finite number of times by P_j. (i.e. a message can be duplicated a finite4 of times, but cannot be duplicated infinitely many times) *(Liveness)*
- *FL3. No creation*: No message is delivered unless it was sent. *(Safety)*
***** Stubborn Links
Channels delivers an message sent infinitely many times.
****** Properties
- *SL1. Stubborn delivery*: If a correct process P_i sends a message m to a correct process P_j, then P_j delivers m an infinite number of times *(Liveness)*
- *SL2. No creation* If a message m is delivered by some process P_j, then m was previsouly sent m some process P_i. *(Safety)*
****** Correctness
- *SL1. Stubborn delivery*: If process does not crash, it will send every message infinitely many times. Messages will be delivered infinitely many times. Fair-loss link may only drop a (large) fraction.
- *SL2. No creation*: Guaranteed by the Fair-loss link
***** Perfect Links
Channels that delivers any message sent exactly once.
****** Properties
- *PL1. Reliable delivery*: If P_i and P_j are correct, then every message sent by P_i to P_j is eventually delivered by P_j. *(Livenss)*
- *PL2. No duplication*: Every message is delivered at most once. *(Safety)*
- *PL3. No creation*: No message is delivered unless it was sent. *(Safety)*
****** Correctness
- *PL1. Reliable delivery*: Guaranteed by Stubborn link. In fact that Stubborn link will deliver it infinite number of times
- *PL2. No duplication*: Guaranteed b our log mechanism
- *PL3. No creation*: Guaranteed by Stubborn link (which garanteed by its fair-loss link)
***** FIFO Perfect Links (Reliable Links)
****** Properties
- *PL1. Reliable delivery*: If P_i and P_j are correct, then every message sent by P_i to P_j is eventually delivered by P_j. *(Livenss)*
- *PL2. No duplication*: Every message is delivered at most once. *(Safety)*
- *PL3. No creation*: No message is delivered unless it was sent. *(Safety)*
- *FFPL. Ordered delivery*: if m_1 is sent before m_2 by P_i to P_j and m_2 is delivered by P_j then m_1 is delivered by P_j before m_2
****** TCP vs FIFO
- TCP provides reliable delivery of packets.
- TCP reliability is so called "session based"

  In general, FIFO Perfect Links is more strict than TCP
**** Timing Assumptions
***** Asynchronus Model and Causality
****** Asynchronus Systems
- *No timing assumption* on processes and channels
  + Processing time varies arbitarily
  + No bound on transmission time
  + Clocks of different processes are not synchronized
- Reasoning in this model is based on which events may cause other events e.g. *(Causality)*
- *Total order* of event *not observable* locally, no accessto global clocks.
****** Causal Order (Happen before)
- If a occurs before b on the same process, then a -> b
- If a id a send(m) and b deliver(m), then a -> b
- a -> b is transitive: i.e. If a -> b and b -> c then a -> c
****** Equivalence of executions
If two executions F and E have the same collection of events, and their causal order is preserved, F and E are said to be *similar executions*, written F~E

*NOTE*: F and E could have different permutaion of events as long as causality is preserved!

******* Computations:
- ~ is reflexive: i.e. a~a for any execution.
- ~ is symmetric. i.e. If a~b then b~a for anyy executions a and b
- ~ is transitive. i.e. If a~b and b~c, then a~c, for any executions a, b, c
******* Computation theorem gives 2 important results:
- There is no algorithm in the asynchronous system model that can observe the *order* of the sequence of events (that can "see" the time-space diagram, or the trace) for all executions
  + Proof:
    - Assume such an algorithm exists. Assume p knows the order in the final (repeated) configuration
    - Take two distinct similar executions of algorithm preserving causality
    - Computation theorem says their final repeated configurations are the *same*, then the algorithm _cannot_ have observed the actual order of events as *theyy differ*
- The computation theorem does not hold if the model is extended such that each process can read a local *hardware clock*
  + Proof:
    - Similarly, assume a distributed algorithm in which each process reads the local clock each time a local event occurs
    - The final (repeated) configuration of different causalit preserving executions will have different clock values, which would contradict the compuration theorem
****** Synchronous Systems
Model assumes:
- *Synchronous computation*:
  + Known upper bound on how long it takes to perform computation
- *Synchronous communication*:
  + Known upper bound on message transmission delay
- *Synchronous physical Clocks*:
  + Nodes have local physical clock
  + Known upper bound clock-drift rate and clock skew
****** Partial Synchrony
Asynchronous sstem, which *eventually* becomes synchronous. Cannot known when, but in every execution, some bounds eventually will hold.

Your algorithm will have along *enough time* window, where everything behaves nicely (synchrony), so that it can achieve its goal.

Noticed that the time at which a system behaves synchronously is *unknown*:
- To *prove safet properties* we need to *assume* that the system is asynchronous.
- To *prove liveness* we use the *partial synchrony* assumption
***** Logical Clocks
****** Lamport logical clock
Lamport clocks guarantee that:
- If a -> b, then t(a) < t(b)
- If t(a) >= t(b), then not(a -> b)

The happen-before relation is a partial order.

In contrast logical clocks are total:
- Information about non-causalit is *lost*
  + We cannot tell by looking to the timestamps of event a and b whether there is a causal relation between the events, or they are concurrent
****** Vector clock
- if v(a)<v(b), then a -> b in addition to
- if a -> b, then v(a) < v(b)

Cons:
- payload is huge.
- When new process join have to reconfigure and update vector

* STRT Lecture 3 - Failure Detectors
** Implementation of Failure detectors
- periodically exchange heartbeat messages
- timeout based on worst case message round trip
- if timeout, then suspect process
- if received message from suspected node, revise suspicion and increase timeout

** Completeness and Accuracy
*Completeness requirements*: Requirements regrading actually crashed nodes. (when do they have to be detected?)

*Accuracy requirements*: Requirements regrading actually alive nodes. (when are they allowed to be suspected?)

In asynchronous system:
- Is it possible to achieve completeness?
  + es, suspect all processes
- Is it possible to achieve accuracy?
  + Yes, refrain from suspecting any process!
- Is it possible to achieve both?
  + No!

Failure detectors are feasible *only* in synchronous and partiall synchronous systems.
*** Strong Completeness
Every crashed process is *eventually* detected by all correct processes.

There exists a time after which all crashed processes are detected byy all correct processes
*** Weak Completeness
Every crashed process is *eventually* detected by some correct [rpcess]

There  exists a time after which all crashed nodes are detected by some correct nodes.
- Possibly detected by *different* correct nodes
- e.g. 4 process P_1 - P_4, P_3 and P_4 crash, and P_1 only detect P_3, P_2 only detect P_4, this is fine!
*** Strong Accuracy
No correct process is *ever* suspected.

achievable in synchronous systems
*** Weak Accuracy
There exists a correct process which is never suspected by any process (well connected node, useful for leader election)
*** Eventual Strong Accuracy
After some finite time the Failure Detector provides *strong accuracy*
*** Eventual Weak Accuracy
After some finite time the Failure Detector provides *weak accuracy*
** Classes of Failure Detectors
4 detectors with *strong completeness*:
- Synchronous Systems:
  + Perfect Detector (P) -> Strong Accuracy
  + Strong Detector (S) -> Weak Accuracy
- Partially Synchronous Systems:
  + Eventually Perfect Detector (<>P) -> Eventual Strong Accuracy
  + Eventually Strong Detector (<>S) -> Eventual Weak Accuracy

4 detectors with *weak completeness*:
- Synchronous Systems
  + Detector Q -> Strong Accuracy
  + Weak Detector (W) -> Weak Accuracy
- Partially Synchronous Systems:
  + Eventually Detector Q (<>Q) -> Eventual Strong Accuracy
  + Eventually Weak Detector (<>W) -> Eventual Weak Accuracy
*** Perfect Failure Detector - P
**** Properties of P
- *PFD1 (strong completeness)*: Eventually every process that crashes is permanently detected by every correct process. *(Liveness)*
- *PFD2 (strong accuracy)*: If a node p is detected by any node, then p has crashed. *(Safety)*
**** Correctness of P
- *PFD1 (strong completeness)*:
  + A crashed process does not send <heartbeat>
  + Eventually every process will notice the absence of <heartbeat>
- *PFD2 (strong accuracy)*:
  + Assuming local compuration is negligible
  + Maximum time between 2 heartbeats
  + If alive, all process will receive <heartbeat> in time. No inaccuracy
*** Eventually Perfect Failure Detector - <>P
**** Properties of <>P
- *PFD1 (strong completeness)*
- *PFD2 (eventual strong accuracy)*: *Eventually*, no correct process is suspected by any correct process
**** Correctness of <>P
- *PFD1 (strong completeness)*: Same as before
- *PFD2 (eventuall strong accuracy)*:
  + Each time p is inaccurately suspected by a correct q
    - Timeout T is increased at q
    - Eventuallyy system becomes synchronous, and T becomes larger than the unknown bound
    - q will receive <heartbeat> on time, and never suspect p again
** Leader Election
Formally, leader election is a Failure Detector:
- Always suspects all process except one (leader)
- Ensures some properties regarding that process
*** Leader election (LE) which "matches" P
**** Properties of LE
- *LE1 (eventual completeness)*: Eventually every correct process trusts some correct process
- *LE2 (agreement)*: No 2 correct processes trusts different correct processes
- *LE3 (local accuracy)*: If a process is elected leader by P_i, all previously elected leaders by P_i have crashed
*** Eventual leader election (omega) which "matches" <>P
**** Properties of ELE
- *ELD (eventual completeness)*: *Eventually* every correct node trusts some correct node
- *ELD2 (eventual agreement)*: *Eventually* no 2 correct nodes trusts different correct node

Can we elect a recovered process?: Not if it keeps crash-recovering infinitely often
** Reduction
We cay  X \preceq Y if:
- X can be solved given a solution of Y
- Read X is reducible to Y
- Informally problem X is easier or as hard as Y

a relation \preceq is a preorder on a set A.
** Combining Abstractions
*** Fail-stop - Synchronous
- Crash-stop process model
- Perfect links + Perfect failure detector (P)
*** Fail-silent - Asynchronous
- Crash-stop process model
- Perfect links
*** Fail-noisy - Partially synchronous
- Crash-stop process model
- Perfect links +  Eventually Perfect failure detector (<>P)
*** Fail-recovery
- Crash-recovery process model
- Stubborn link + ...
* STRT Lecture 4 - Reliable Broadcast
** Combining Abstractions
*** Combining Abstractions
*** Fail-stop - Synchronous
- Crash-stop process model
- Perfect links + Perfect failure detector (P)
*** Fail-silent - Asynchronous
- Crash-stop process model
- Perfect links
- No access to failure detectors!
*** Fail-noisy - Partially synchronous
- Crash-stop process model
- Perfect links +  Eventually Perfect failure detector (<>P)
- To guarantee safety properties an algorithm has to assume the failure detector inaccurate
- Eventual accuracy is only used to guarantee liveness
*** Fail-recovery
- Crash-recovery process model
- Stubborn link or a persistent links (logs)
- Relies often on a persistent memory to store and retrive crtical info.
** Quorums
- Quorum is any set of majority of processes.
- The algorithms will rely on a majority of processes will not fail
  + f < N/2 (f is the max number of faulty processes, AKA the *resilience* of the algorithm)
** Broadcast Abstractions
*** Reliable broadcast abstractions
- *Best-effort broadcast*: Guarantees reliabilit only if sender is correct
- *Reliable broadcast*: Guarantees reliability independent of whether sender is correct
- *Uniform reliable broadcast*: Also considers behaviour of failed nodes
- *FIFO reliable broadcast*: Reliable broadcast with FIFO delivery order
- *Causal reliable broadcast*: Reliable broadcast with causal delivery order
- *Probabilistic reliable broadcast*:
  + Guarantees reliability with high robability
  + Scales to large number of nodes
- *Total order (atomic) reliable broadcast*: Guarantees reliability and same order of delivery
*** Best-Effort Broadcast (BEB)
Use Perfect channel abstraction ->  Upon <beb Broadcast | m> send message m to all processes (for loop)
**** Properties
- *BED1. Best-effor-Validity*: If P_i and P_j are correct, then any broadcast by P_i is eventually delivered by P_j
- *BEB2. No duplication*: No message delivered more than once
- *BEB3. No creation*: No message delivered unless broadcast
**** Correctness
- If sender does not crash, every other correct process receives message by perfect channels *(Validity)*
- *No creation* & *No duplication* already guaranteed by perfect channels.
*** Reliable Broadcast (RB)
Same as BEB, plus if sender crashes: ensure all or none of the correct nodes get msg
**** Properties
- *RB1 = BEB1. Validity*: If *correct* P_i broadcasts m, P_i itself eventually delivers m
- *RB2 = BEB2. No duplication*
- *RB3 = BEB3. No creation*
- *RB4. Agreement*: If a *correct node delives* m, then every correct process delivers m
**** Fail-Stop: Lazy Reliable Broadcast
Requires perfect failure detector (P) and Beb broadcast

To reliable broadcast m:
- beb boardcast m
- When get beb Deliver
  + save message, and *RB* Deliver message
- If sender s crash, detect and relay message from s to all
***** Correctness of Lazy RB
- *RB1-RB3* satisfied by BEB
- Need to prove *RB4*: If a *correct node delivers* m, then every correct node delivers m.

Assume correct P_k delivers message bcast by P_i:
- If P_i is correct, BEB ensures correct delivery
- If P_i crashes,
  + P_k detects this. (completeness)
  + P_k uses BEB to ensure (BEB1) every correct node gets it
***** Complexity of Lazy RB
Assume N processes:
- *Message complexity*:
  + Best case: O(N) messages
  + Worst case: O(N^2) messages
- *Time complexity*:
  + Best case: 1 time unit
  + Worst case: 2 time units
**** Fail-Silent Eager Reliable Broadcast
uses <>P instead of P. This only affects performance. -> Since we have to assume all processes failed (*worst case senario of Lazy RB*), and we BEB broadcast as soon as you get a message.
*** Uniform Reliable Broadcast
If a *failed process* delivers a message m, then every correct node delivers m.
**** Properties
- *URB1 = RB1*
- *URB3 = RB2*
- *URB3 = RB3*
- *URB4. Uniform Agreement*: For any message m, if a process delivers m, then every correct process delivers m
**** Uniform Eager RB
- Messages and *pending* until all correct processes get it.
  + Collect ACKs from processes that got msg
- Deliver once all correct processes acked
  + Use perfect FD (P)
***** Correctness of Uniform RB
- *No creation* from BEB
- *No duplication* by using "delivered" set
- *Lemma*: If a *correct* process P_i bebDelivers m, then P_i evnetually urbDelivers m
  + Proof:
    - Correct process P_i bebBroadcasts m as soon as it gets m
      + By *BEB1* every correct process gets m and bebBroadcasts m
      + P_i get bebDeliver(m) from every correct process by BEB1
      + By completeness of P, it will not wait for dead nodes forever
        - *canDeliver(m) becomes true and P_i delivers m
****** Validity
If sender is correct, it will by *validity (BEB1)* bebDeliver m.

By the *lemma*, it will eventually urbDeliver(m)
****** Uniform agreement
- Assume some process (possibly failed) URB delivers m
  + Then canDeliver(m) was true, by *accuracy* of P *every* correct process has BEB delivered m
- By *lemma* each of the nodes that BEB delivered m will URB deliver m
**** Fail-Silent Majority-ACK Uniform RB
Assume a majority of correct nodes. Use same algorithm as uniform eager RB, but wait for a majority of nodes has acknowledged m.

- Agreement
  + If a process URB delivers, it got ack from majority
  + In that majorit, one node, p, must be correct
  + p will ensure all correct processes BEB deliver m
    - The correct processes (majority) will ack and URB deliver
- Validity
  + If correct sender sends m
    - All correct nodes BEB deliver m
    - All correct nodes BEB broadcast
    - Sender receives a majorit of acks
    - Sender URB delivers m
** Resilience
The maximum number of fault processes an algorithm can handle.
- *Fail-Silent algorithm* has resilience less than N/2
- *Fail-Stop algorithm* has resilience N-1
** Casual Broadcast
*** Causalit of Messages
- *C1 (FIFO order)*: Some process P_i broadcasts m_1 before broadcasting m_2
- *C2 (Network order)*: Some process P_i delivers m_1 and later broadcasts m_2
- *C3 (Transitivity)*: There is a message m such that m_1 -> m and m -> m_2
*** Reliable Causal Broadcast
**** Properties
- *RB1-RB4* from regular reliable broadcast
- *CB*: If node P_i delivers m, then P_i must deliver ever message causally preceding (->) m before m
*** Uniform Reliable Causal Broadcast
**** Properties
- *URB1-URB4* from uniform reliable broadcast
- *CB*: If node P_i delivers m, then P_i must deliver ever message causally preceding (->) m before m
*** Fail-Silent Waiting Causal Broadcast
Represent past history bu *vector clock (VC)*, Piggback VC and RB broadcast m
- Upon RB delivery of m with attached VC_m
- compare VC_m with local VC
  + Only deliver m once VC_m <= VC
  + Do not deliver if VC_m > VC or VC_m != VC
**** Agreement (Correctness)
- Assume m is co-delivered at correct pi
- pi co-delivered all message causall before m
- Every correct process rb-delivered m and all causall preceding messages (agreement of RB)
- Hence every correct process co-deliver m
*** Other possible orderings
**** Single-source FIFO order
Msgs from same node delivered in order sent
***** Caveat
This formulation does not require delivery of both messages
**** Total Order
Everone delivers everything in exact same order
***** Caveat
This formulation does not require deliver of both messages.

Everyone delivers same order, mabe not send order!
* STRT Lecture 5 - Distributed Shared Memory
** (1, N) Regular register
Validity:
- Read returns *last value written* if
  + Not concurrent with another write, and
  + Not concurrent with a failed write
- Otherwise ma return last or concurrent value
*** Fail-Stop Read one Write all (1, N)
Fail-stop model, use perfect FD (P)
- to write(v):
  + Update local value to v
  + Broadcast v to all
  + Wait for ACK from all correct processes
  + Return
- to read
  + Return local value
**** Correctness
Assume we use BEB broadcast, Perfect links and P

*validity*:
- No concurrent write with the read operations
  + Assume p invokes a read, and v last written value
  + At time of read by p, the write is complete (accurac of P) and p has v stored locally
- Read is concurrent with write of value v, v' the value prior to v
  + Each process store v' before write(v) is invoked
  + At a read is invoked each process either stores v or v'
  + As the writes is concurrent, either value is correct to read
**** Performance and Resilience
- Time complexity (write): 2 communication steps (broadcast and ACK)
- Message complexity: O(N) messages
- Resilience: fault processes f = N - 1
*** Fail-Silent Majority Voting Algorithm
*Quorum principle*:  Always write and read from a majority of processes. At least one correct process knows most recent value
- *PRO*: Tolerate up to N/2 -1 crashes
- *CON*: Have to read/write N/2 +1 values
**** Correctness
- No concurrent write with the read operations
  + Assume q invokes a read, and (ts, v) last written value by p. ts is the highest time stamp
  + At time of read-inv b q, a majority has (ts, v)
  + q gets at least one response with (ts, v) and returns v
- Read is concurrent with a write with value (ts, v)
  + (ts-1, v') the value prior to (ts, v)
  + Majority of process store (ts-1, v') before write(v) is invoked
  + The query phase of the read returns either (ts-1, v') or (ts, v)
**** Performance and Resilience
- Time complexit (both write and read):  2 communication steps (one round trip)
- Message complexity: O(N) messages
- Resilience: faulty processes f < N/2
** Atomic/Linearizability vs Sequential Consistency
Linearizability > Sequential Consistency & Regular.  But Regular != Sequential
*** Sequential consistency
Definition: The result of any execution is the *same* as if the operations of all the process were executed in *some sequential order*, and the operations of each *individual process* in this sequence are in *the order specified by its program*.

Only allow executions whose results appear as if there is a single system image and "local time" is obeyed.
*** Linearizability / Atomic Consistency
Definition: The result of any execution is the *same* as if the operations of all the processes were executed in *some sequential order*, and the operations in this sequence are in the *global time order of operations* (Occurs between invocation and response)

Only allow executions whose results appear as if there is a single system image and "global time" is obeyed
* STRT Lecture 6 - Atomic Registers + Sequentiall Consistent Algorithm
** Liveness: progress
*** Wait-free
Every corret node should "make progress"
- No deadlocks
- No live-locks
- No starvation
*** Lock-free/non-blocking
At least one correct node should "make progress"
- No deadlocks
- No live-locks
- Mabe starvation
*** Obstruction free/ solo-termination
If a single node executes without interference (contention) it makes progress
- No deadlocks
- Mabe live-locks
- Mabe starvation
** Atomic Register
*** Termination (Wait-Freedom)
If node is correc, each read and write op eventuall completes
*** Linearization Points
- *Read ops* appear as if immediately happened at all nodes at
  + time between invocation and response
- *Write ops* appear as if immediately happened at all nodes at
  + time between invocation and response
- *Failed ops* appear as
  + completed at every node, XOR
  + never occurred at any node
*** (1, N) Atomic Algorithm: Read-impose write majority
Same as (1, N) regular register, but to read:
- Broadcast read request to all
  + Receiver respond with local value v and ts (quer phase)
- Wait and save values from majority of nodes
- Perform an update phase with highest (ts, v)

Optimization:
- If all responses in the query phase have the same ts, do not perform the update phase, just return
- This is because a majorit has the latest value written.

**** Validity
- Read returns last value written if Not concurrent with another write. Not concurrent with a failed operation
- Otherwise ma return last or concurrent "value"

A read ops r1 makes an update with r1, so an succeeding read must at least see r1
*** (N, N) Atomic Algorithm: Read-impose write-consult-majority
- A write to complete requires 2 round trips of messages
  + One for getting the latest timestamp (query phase)
  + One for broadcast - ACK (Update phase)
- A read to complete requires 2 round trip max:
  + One for read (query phase)
  + One for impose if necessar (update phase)
*** (N, N) algorithm for Sequentially consistent registers:  Logical Time (LT) algorithm
In Fail-silent model implement read/write multiple register shared memory.
- Multiple writers and multiple readers
- Sequentially consistent model
- Writes in 1 RTT and reads in 2 RTTs
- Tolerates f < N/2 faulty processes failing b crashing
**** compositionality of LT algorithm
- LT-algorithm linerizes read/writes in logical time instead of real time.
- Executions (real time traces) are sequentiall consistent instead of linerizable
- *Linerizability in logical time allows compositionality*
- LT-algorithm satisfies SC, but also satisfies a *bit stronger consistenc condition* that is *compositionality*

[[./id2203-img/SM.PNG]]
* STRT Lecture 7 - Single Value Uniform Consensus - Paxos
** Single value consensus
- *C1.Validity*: An value decided is a value proposed
- *C2.Agreement*: No two correct nodes decide differently
- *C3.Termination*: Every correct node eventually decides
- *C4.Integrity*: A node decides at most once
** Single value uniform consensus
- *Validity*: Only proposed values may be decided
- *Uniform Agreement*: No two process decide different values
- *Integrity*: Each processes can decide a value at most once
- *Termination*: Ever process eventually decides a value

Solvable in Fail-Stop model (decide on last round) with strong FD

*Not Solvable* in Fail-Silent model (asynchronous system model)
** Paxos
*** Assumptions
- Partially synchronous system
- Fail-noisy model
- Message duplication, loss, re-ordering
*** High Level View of Paxos
- Elect a *single proposer* using Omega
  + Proposer imposes its proposal to everyone
  + Everyone decides
- Problem with Omega
  + Several processes might initiall be proposers (contention)
- Solution is *Abosrtable Consensus*
  + Processes attempt to impose their proposals
  + Might *abort* if there is contention *(safety)* (multiple proposers)
  + Omega ensures eventuall 1 proposer succeeds *(liveness)*
*** The Paxos Algorithm
**** Terminology
- *Proposers*: Will attempt imposing their *proposal* to set of acceptors
- *Acceptors*: May *accept* values issued by proposers. Acceptors can not talk to each other
- *Learners*: Will *decide* depending on acceptors acceptances

Each process plays all 3 roles in classic setting
**** Abortbale Consensus
- Abortable Consensus in a nutshell
  + *P1.*: An acceptor *accepts* first proposal it receives
  + *P2.*: If v is *chosen*, every higher proposal *chosen* has value v
  + *P2a.*: If v is *chosen*, every higher proposal *accepted* has value v
  + *P2b.*: If v is *chosen*, every higher proposal *issued* has value v
  + *P2c.*: If any proposal(n, v) is issued, there is a majority set S of acceptors such that either
    - (a). no one in S has *accepted* an proposal numbered less than n
    - (b). v is the value of the highest proposal among all proposals less than n *accepted* b acceptors in S

- Handwaving:
  + P1 ensures *obstruction-free progress* and *validity*
  + P2 ensures *agreement*
  + Integrity is trivial to implement
    - Remember if chosen before, at most choose once.

[[./id2203-img/PC1.PNG]]

*NOTE*: You do not propose if you are not the leader. need both Omega + Abortable Consensus to reach Consensus
***** Optimization
[[./id2203-img/PC2.PNG]]

- Paxos require 2 round trips (with no contention)
  + Prepare(n): prepare phase (read phase)
  + Accept(n, v): accept phase (write phase)
- P2. If v is *chosen*, every higher proposal *chosen* has value v
  + *Final Optimization*: Proposer skips the accept phase if a majorit of acceptors return the same value v
***** State to Remember
[[./id2203-img/PC3.PNG]]
Can run in Fail-recovery model
***** Performance
- Paxos requires 4 message delays (2 RTTs)
  + Prepare(n) needs 2 delays (Broadcast & Get Majority)
  + Accept(n, v) needs 2 delays (Broadcast & Get Majority)
- In man cases onl accept phase is run
  + Paxos only needs *2 delays* to terminate
  + (Believed to be) optimal
* STRT Lecture 8 - Replicated State Machines & Sequence Consensus
**  State Machine
- Executes a sequence of commands
- Transform its state and may produce some output
- Commands are deterministric
  + Outputs of the state machine are solely determined by the initial state and by the sequnece of commands that it has executed
** RSM
- Replicated log ensures state machines execute same commands in same order
- Consensus module guarantees agreement on command sequence in the replicated log
- System makes progress as long as any majority of servers are up
** Sequence Consensus
In single value paxos, a process can decide at most one value. so we can extend it with more information since we do not want to change what has been decided before.
- This is allowed by Sequence Consensus
  + Can decide again if old decided sequence is a prefix of the new one
*** Sequence consensus properties
- *Validity*: If process p decides v then v is a *sequence* of proposed commands (*without duplicates*)
- *Uniform Agreement*: If process p decides u and process q decides v, then one is a prefix of the other
- *Integrity*: If process p decides u and later decides v then *u is a strict prefix of v*
- *Termination*: If command C is proposed by a correct process then eventually every correct process decides a sequence containing C. (Liveness)
** Sequence Paxos
Same as vanilla paxos, we are only making 2 changes:
- After adopting a value (seq) with highest proposal number, the proposer is allowed to extendtje sequence with (nonduplicate) new command(s)
- Learner that receives <Decide, v> will decide v if v is longer sequence than previously decided sequence.
*** Correctness & Efficiency
- Correctness in modeled after the single value paxos correctness prood
- Efficiency?
  + Every proposal takes 2 round trips
  + Proposals are not pipelined
  + Sequences are send back and forth
  + Decide carries sequences

*A dequence v is chosen at round n*: If there exists an quorum Q of acceptors at round n such that v is prefix Va[p,n] for every acceptor p in Q

*A sequence v is chosen*: If v is chosen at n, for some round n

NOTE: In prepare phase the acceptor send back the latest chosen sequence
* STRT Lecture 9 - Leader-Based Sequence Paxos
** Ballot Leader Election
- BLE satisfies completness and eventually accuracy
- and also monotonically unique ballots
*** Propoerties
- *BLE1. completeness*: Eventually every corrrect process elects some correct process if a majority is correct
- *BLE2. eventual agreement*: Eventually no two correct processes elect different correct processes
- *BLE3. monotonic unique ballots*: If a process L with ballot n is elected as leader by P_1, all previously elected leaders by P_i have ballot numbers less than n, and (L, n) is a unique number.

Desirable properties:
- We will allow a process p to "inaccurately" leave a correct leader as long as the new leader has a higher ballot number
- We will also require that a process is elected as a leader only if a majority of processes are correct and alive. This fits sequence paxos
*** Assumptions
- We assume initially a fail-noisy model
  + Processes fail by crashing
  + Initial arbitrary network delays but eventually stabilizes (partially synchronous system)
  + Perfect point-to-point links
- However the algorithms works for a weaker model where the network may drop messages and processes crash and recover
*** Basic idea
- Ballots are unique
  + Each process p has its own ballot (n, pid)
  + A ballot is the rank of a process
- Max ballot is available at each correct process
  + Each correct process periodically gossips its ballot to all processes
- Processes are ranked
  + Eventually each correct process will elect the process with the highest rank (max ballot) given good network conditions (eventual agreement)

- Majority requirment
  + Each correct process will trust a leader only if the leader's max ballot is among the collected ballots from a majority of processes
- Monotonically increasing ballots
  + Every process p that do not receive the leader's ballot (n, pid) among collected ballots consider the leader has crashes
  + p increases his own ballot (n+1, pid)
BLE3 is satisfied and also BLE1 assuming eventual synchrony
*** Conclusion on BLE
[[./id2203-img/PC4.PNG]]
** Prepare once and pipeline accept
for more info, check [[./id2203-img/lecture11.pdf][Lecture slides]]

Also [[arxiv.org/pdf/2008.13456.pdf][paxos-lecture-notes]] is useful
*** Current sequence Paxos is inefficient
- With multiple concurrent proposers, conflicts and restarts are likely (higher load -> more conflicts)
- 2 rounds of messages for each value chosen (Prepare, Accept)
*** Solution
- Pick a Leader(L, n) where n is a unique higher round number (leader election algorithm)
- The leader acts as solo proposer for round n
- After first Prepare (if not aborted) only perform accepts until aborted by another Leader(n') where n' > n
*** Benefit
- Proposer does prepare(n) before first accept(n,v)
- After that only one round trip tp decide on an extension of sequence v, as long as round is not aborted
- (new leader with higher number)
- Allows multiple outstanding accept requests (pipelining)
  + Lower propose-to-decide latency for multiple proposals
* STRT Lecture 10 - Raft for consistent replicated log
** Fail-Recovery in Sequence Paxos
- In the fail recovery model a process is correct as long as it fails (by crashing) and recovers finite number of times
- By crashing and restarting a process p loses any arbitrary suffixes of most recent messages in each FIFO link
- Once a process restart: it joins the leader-election algorithm in a *recover* state
*** Fail Recovery persistent variables
- The algorithm needs to store the following variables in a persistent store for each process
  + *n_prom*: Promise not to accept in lower rounds
  + *n_a*: Round number in which last command is accepted
  + *v_a*: Accepted sequence
  + *l_d*: Length of decided sequence
- A recovered process resets its ballot_max to n_prom in BLE
- The leader election guarantees that a leader with high ballot is elected if the leader crashed and recovered
** Raft Consensus Algorithm
*** Terminology
- Sequence Paxos = Raft
- v_a the accepted sequence = The log
- The decided sequence = The committed prefix of Log
- Round/ballot number = Term
- Process = Server
- n_prom, n_L = Highest Term
- Element in a sequence = Entry

Note: as long as our ballot is higher, we could win LE even though our seq is low in Sequence Paxo, but that is not the case in Raft
*** Summary
- Raft as Sequence Paxos have the same basic paxos idea
  + The longest chosen sequence is the decided (committed) sequence
  + Leader must have a higher round (term) number
- Raft differs from Sequence Paxos on
  + Leader election algorithm (has to be BLE in Raft)
  + Incorporating the prepare phase as part of electing a leader
  + Log (Accepted Sequence) reconciliation between leaders and followers
** Reconfiguration
 A process have replicas in multiple configurations, but can *only be running in one* configuration at any time.

 See lecture slides for more detail info
*** Summary
- Reconfiguring a replicated state machine is relatively straight forward.
- Round numbers are extended so that rounds in an earlier configuration are order befroe rounds in a later configuration
- At most one of the replicas that a process implements maybe running at any time
- The hand-over procedure is important in order to get availability and efficiency
* TODO Lecture 11 -
* TODO Lecture 12 -
